{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dtian09/MLI_Training/blob/master/ViT_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y torch torchvision torchaudio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZddKKsRvpKX",
        "outputId": "0d3a057b-95a0-464d-a358-22ac594bdeeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torch 2.6.0+cu124\n",
            "Uninstalling torch-2.6.0+cu124:\n",
            "  Successfully uninstalled torch-2.6.0+cu124\n",
            "Found existing installation: torchvision 0.21.0+cu124\n",
            "Uninstalling torchvision-0.21.0+cu124:\n",
            "  Successfully uninstalled torchvision-0.21.0+cu124\n",
            "Found existing installation: torchaudio 2.6.0+cu124\n",
            "Uninstalling torchaudio-2.6.0+cu124:\n",
            "  Successfully uninstalled torchaudio-2.6.0+cu124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjLUmFNxuKu5",
        "outputId": "89ffe7e0-fcc5-4da5-f463-e1dffc065246"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Collecting torch\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torch-2.7.0%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (28 kB)\n",
            "Collecting torchvision\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.22.0%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting torchaudio\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.7.0%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Collecting sympy>=1.13.3 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.8.89 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_nvrtc_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (23.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m111.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.8.89 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (875 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m875.6/875.6 kB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.8.87 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux1_x86_64.whl (13.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m128.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==9.1.0.70 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cudnn_cu11-9.1.0.70-py3-none-manylinux2014_x86_64.whl (663.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m663.9/663.9 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.11.3.6 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux1_x86_64.whl (417.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu11==10.3.0.86 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_curand_cu11-10.3.0.86-py3-none-manylinux1_x86_64.whl (58.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.1.48 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux1_x86_64.whl (128.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.5.86 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux1_x86_64.whl (204.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.1/204.1 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu11==2.21.5 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nccl_cu11-2.21.5-py3-none-manylinux2014_x86_64.whl (147.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu11==11.8.86 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nvtx_cu11-11.8.86-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==3.3.0 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.0->torch) (75.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading https://download.pytorch.org/whl/cu118/torch-2.7.0%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl (955.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m955.6/955.6 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu118/torchvision-0.22.0%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl (6.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m124.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/cu118/torchaudio-2.7.0%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m118.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m135.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, sympy, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.1\n",
            "    Uninstalling sympy-1.13.1:\n",
            "      Successfully uninstalled sympy-1.13.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.0+cu118 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu11-11.11.3.6 nvidia-cuda-cupti-cu11-11.8.87 nvidia-cuda-nvrtc-cu11-11.8.89 nvidia-cuda-runtime-cu11-11.8.89 nvidia-cudnn-cu11-9.1.0.70 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.3.0.86 nvidia-cusolver-cu11-11.4.1.48 nvidia-cusparse-cu11-11.7.5.86 nvidia-nccl-cu11-2.21.5 nvidia-nvtx-cu11-11.8.86 sympy-1.13.3 torch-2.7.0+cu118 torchaudio-2.7.0+cu118 torchvision-0.22.0+cu118 triton-3.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0a26ff1-5498-4010-9a5e-7b20bc6ed2af",
        "outputId": "17c8ffb2-02fb-4858-8d0b-aeaa40fa5177",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn                    # For building neural networks\n",
        "import torch.optim as optim              # For optimization algorithms\n",
        "import torch.nn.functional as F          # For activation functions and other utilities\n",
        "from torchvision import datasets, transforms  # For loading and transforming datasets\n",
        "from torchvision.transforms.v2 import ToImage, ToDtype\n",
        "from torch.utils.data import DataLoader, random_split  # For data loading and batching\n",
        "import matplotlib.pyplot as plt          # For plotting\n",
        "import numpy as np\n",
        "import math\n",
        "#import fiftyone as fo\n",
        "\n",
        "# Check if GPU is available and set the device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJYHF2wypD66",
        "outputId": "4d047225-c118-4bd4-d897-413a9ca9306a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fe572bd-f858-4d41-964b-6fdd18e0c228"
      },
      "outputs": [],
      "source": [
        "# # Define transformations for the training and testing data\n",
        "# transform = transforms.Compose([\n",
        "#     transforms.ToTensor(),                # Convert images to PyTorch tensors\n",
        "#     transforms.Normalize((0.5,), (0.5,))  # Normalize images to [-1, 1]\n",
        "# ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8548ba7e-9c47-414f-bdd6-540cae595331"
      },
      "outputs": [],
      "source": [
        "# # Download and load the training data - I have amended this to be for MNIST rather than Fashion MNIST\n",
        "# train_dataset = datasets.MNIST(\n",
        "#     root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "# # Download and load the test data\n",
        "# test_dataset = datasets.MNIST(\n",
        "#     root='./data', train=False, download=True, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Train/Test/Validation Split"
      ],
      "metadata": {
        "id": "sgFqNTJ4DvzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Load and split MNIST into training and validation sets\n",
        "def load_and_split_mnist(validation_split=0.2, batch_size=16):\n",
        "    # Load MNIST dataset\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    mnist_train = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "    mnist_test = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "    # Calculate sizes for split\n",
        "    val_size = int(len(mnist_train) * validation_split)\n",
        "    train_size = len(mnist_train) - val_size\n",
        "\n",
        "    # Split dataset\n",
        "    train_dataset, val_dataset = random_split(mnist_train, [train_size, val_size])\n",
        "\n",
        "    # Create data loaders\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "    test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=False)\n",
        "    return train_loader, val_loader, test_loader\n"
      ],
      "metadata": {
        "id": "42DN2M8_NdJT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 2. Create a function to divide images into patches\n",
        "def image_to_patches(images, patch_size=7):\n",
        "    \"\"\"\n",
        "    Convert batch of images into patches\n",
        "\n",
        "    Args:\n",
        "        images: tensor of shape [batch_size, channels, height, width]\n",
        "        patch_size: size of each square patch\n",
        "\n",
        "    Returns:\n",
        "        patches: tensor of shape [batch_size, num_patches, patch_size*patch_size]\n",
        "    \"\"\"\n",
        "    batch_size, channels, height, width = images.shape\n",
        "\n",
        "    # Calculate number of patches in each dimension\n",
        "    patches_h = height // patch_size\n",
        "    patches_w = width // patch_size\n",
        "\n",
        "    # Reshape images to extract patches\n",
        "    # First reshape to separate patches\n",
        "    patches = images.view(batch_size, channels, patches_h, patch_size, patches_w, patch_size)\n",
        "\n",
        "    # Permute dimensions to group patch dimensions\n",
        "    patches = patches.permute(0, 2, 4, 1, 3, 5)\n",
        "\n",
        "    # Reshape to [batch_size, num_patches, channels*patch_size*patch_size]\n",
        "    patches = patches.reshape(batch_size, patches_h * patches_w, channels * patch_size * patch_size)\n",
        "\n",
        "    return patches\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2uoKf1NkNMNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Create a simple dimensionality reduction module\n",
        "class PatchDimReducer(nn.Module):\n",
        "    def __init__(self, in_features=49, out_features=32):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(in_features, out_features)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: [batch_size, num_patches, patch_size*patch_size]\n",
        "        return self.linear(x)\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # 1. Load and split the dataset\n",
        "    train_loader, val_loader, test_loader = load_and_split_mnist()\n",
        "\n",
        "    # 2. Create the dimension reducer model\n",
        "    dim_reducer = PatchDimReducer(in_features=49, out_features=32)\n",
        "\n",
        "    # Process a batch\n",
        "    for images, labels in train_loader:\n",
        "        # Shape of images: [batch_size, 1, 28, 28]\n",
        "\n",
        "        # Convert images to patches\n",
        "        patches = image_to_patches(images, patch_size=7)\n",
        "        # Shape of patches: [batch_size, 16, 49]  (16 patches of size 7x7=49)\n",
        "\n",
        "        # Apply dimension reduction\n",
        "        reduced_patches = dim_reducer(patches)\n",
        "        # Shape of reduced_patches: [batch_size, 16, 32]\n",
        "\n",
        "        print(f\"Original image shape: {images.shape}\")\n",
        "        print(f\"Patches shape: {patches.shape}\")\n",
        "        print(f\"Reduced patches shape: {reduced_patches.shape}\")\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "gjJelkNCOFaO",
        "outputId": "f12ec4af-c7bb-4c9c-ea00-34f88ee5d040"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:01<00:00, 5.11MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 134kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.27MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 9.92MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original image shape: torch.Size([16, 1, 28, 28])\n",
            "Patches shape: torch.Size([16, 16, 49])\n",
            "Reduced patches shape: torch.Size([16, 16, 32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# For visualization of the patches (optional)\n",
        "def visualize_patches(image, patch_size=7):\n",
        "    \"\"\"Visualize the patches created from a single image\"\"\"\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np\n",
        "\n",
        "    # Convert to numpy and squeeze channel dimension for plotting\n",
        "    image_np = image.squeeze().numpy()\n",
        "\n",
        "    # Calculate patches per dimension\n",
        "    patches_h = image_np.shape[0] // patch_size\n",
        "    patches_w = image_np.shape[1] // patch_size\n",
        "\n",
        "    # Create a figure to display the patches\n",
        "    fig, axes = plt.subplots(patches_h, patches_w, figsize=(8, 8))\n",
        "\n",
        "    # Extract and plot each patch\n",
        "    for i in range(patches_h):\n",
        "        for j in range(patches_w):\n",
        "            patch = image_np[i*patch_size:(i+1)*patch_size, j*patch_size:(j+1)*patch_size]\n",
        "            axes[i, j].imshow(patch, cmap='gray')\n",
        "            axes[i, j].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Tyq3rf1iOI2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a batch from a dataloader\n",
        "dataiter = iter(train_loader)\n",
        "batch_images, batch_labels = next(dataiter)\n",
        "\n",
        "# Access the 5th image in this batch\n",
        "image_5 = batch_images[4]  # Note: zero-indexed\n",
        "label_5 = batch_labels[4]\n",
        "plt.imshow(image_5.squeeze(), cmap='gray')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "3xhkuSf4aHNj",
        "outputId": "ed08ecbe-fd59-4f39-bec3-a41feaecb312"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7ae2e05a76d0>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_patches(images[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        },
        "id": "MzAS0H5UOhQV",
        "outputId": "9f8958ed-4e14-482a-abee-9555be2b48ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 16 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For 2D image patches, you could also create a specialized version:\n",
        "class LearnablePosition2DEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    Learnable 2D position embeddings for image patches.\n",
        "    \"\"\"\n",
        "    def __init__(self, height, width, embed_dim):\n",
        "        super().__init__()\n",
        "        self.height = height\n",
        "        self.width = width\n",
        "\n",
        "        # Initialize separate embeddings for rows and columns\n",
        "        # This gives the model more structure about 2D positions\n",
        "        self.row_embed = nn.Parameter(torch.zeros(height, embed_dim // 2))\n",
        "        self.col_embed = nn.Parameter(torch.zeros(width, embed_dim // 2))\n",
        "\n",
        "        # Initialize with small random values\n",
        "        nn.init.trunc_normal_(self.row_embed, std=0.02)\n",
        "        nn.init.trunc_normal_(self.col_embed, std=0.02)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Add 2D position embeddings to patch embeddings\n",
        "        Args:\n",
        "            x: Patch embeddings of shape [batch_size, height*width, embed_dim]\n",
        "        Returns:\n",
        "            x with position embeddings added\n",
        "        \"\"\"\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Create 2D position embeddings\n",
        "        pos_embed = torch.zeros(batch_size, self.height * self.width, x.size(2),\n",
        "                                device=x.device, dtype=x.dtype)\n",
        "\n",
        "        # Fill the position embeddings\n",
        "        for i in range(self.height):\n",
        "            for j in range(self.width):\n",
        "                pos = i * self.width + j\n",
        "                pos_embed[:, pos, :self.row_embed.size(1)] = self.row_embed[i]\n",
        "                pos_embed[:, pos, self.row_embed.size(1):] = self.col_embed[j]\n",
        "\n",
        "        return x + pos_embed"
      ],
      "metadata": {
        "id": "wRvORP7svKrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Encoder Only Model for Digit Classification\n"
      ],
      "metadata": {
        "id": "7hjN19r_Dlrs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# start of with just a single encoder block and a single attention mechanism\n",
        "class TransformerEncoderBlockOld(nn.Module):\n",
        "    def __init__(self, embed_dim, hidden_dim=128, dropout=0.1, num_heads=1):\n",
        "        super().__init__()\n",
        "\n",
        "        # Save embedding dimension for scaling\n",
        "        self.embed_dim = embed_dim\n",
        "        self.head_dim = 24\n",
        "\n",
        "        # Self-attention components\n",
        "        self.query = nn.Linear(embed_dim, self.head_dim).float()\n",
        "        self.key = nn.Linear(embed_dim, self.head_dim).float()\n",
        "        self.value = nn.Linear(embed_dim, self.head_dim).float()\n",
        "\n",
        "        # Output projection from attention\n",
        "        self.attn_proj = nn.Linear(self.head_dim, embed_dim).float()\n",
        "\n",
        "        # Layer normalization\n",
        "        self.norm1 = nn.LayerNorm(embed_dim).float()\n",
        "        self.norm2 = nn.LayerNorm(embed_dim).float()\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # MLP block\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embed_dim, hidden_dim).float(),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, embed_dim).float(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        # Classification head\n",
        "        self.classifier = nn.Linear(embed_dim, 10).float()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Store original input for residual connection\n",
        "        residual = x\n",
        "\n",
        "        # Self-attention\n",
        "        q = self.query(x)\n",
        "        k = self.key(x)\n",
        "        v = self.value(x)\n",
        "\n",
        "        print(f\"I'm in the Encoder forward pass, q, k and v are of shape{q.shape}, {k.shape}, {v.shape}\")\n",
        "\n",
        "        # Attention calculation\n",
        "        attn_scores = torch.bmm(q, k.transpose(1, 2)) * (self.head_dim ** -0.5)\n",
        "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)  # Apply dropout to attention weights\n",
        "        attn_output = torch.bmm(attn_weights, v)\n",
        "\n",
        "        # Project attention output\n",
        "        attn_output = self.attn_proj(attn_output)\n",
        "        attn_output = self.dropout(attn_output)\n",
        "\n",
        "        # Add residual connection and apply layer norm\n",
        "        x = self.norm1(residual + attn_output)\n",
        "\n",
        "        # MLP block with residual connection\n",
        "        residual = x\n",
        "        mlp_output = self.mlp(x)\n",
        "        x = self.norm2(residual + mlp_output)\n",
        "\n",
        "        # Mean pooling across patches for classification\n",
        "        x = x.mean(dim=1)  # Shape: [batch_size, embed_dim]\n",
        "\n",
        "        # Final classification layer\n",
        "        logits = self.classifier(x)\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "vnFwETtyNLqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with multi-head capabilities\n",
        "class TransformerEncoderBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, hidden_dim=128, dropout=0.1, num_heads=2, head_dim=24):\n",
        "        super().__init__()\n",
        "\n",
        "        # Store attention dimensions\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = head_dim\n",
        "        self.total_head_dim = num_heads * head_dim\n",
        "\n",
        "        # Combined QKV projection for all heads\n",
        "        self.qkv_proj = nn.Linear(embed_dim, 3 * num_heads * head_dim).float()\n",
        "\n",
        "        # Output projection from attention\n",
        "        self.attn_proj = nn.Linear(num_heads * head_dim, embed_dim).float()\n",
        "\n",
        "        # Store dimensions for reshaping\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        # Layer normalization\n",
        "        self.norm1 = nn.LayerNorm(embed_dim).float()\n",
        "        self.norm2 = nn.LayerNorm(embed_dim).float()\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # MLP block\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embed_dim, hidden_dim).float(),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, embed_dim).float(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        # Classification head\n",
        "        self.classifier = nn.Linear(embed_dim, 10).float()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Store original input for residual connection\n",
        "        residual = x\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "\n",
        "        # Combined QKV projection\n",
        "        qkv = self.qkv_proj(x)  # Shape: [batch_size, seq_len, 3 * num_heads * head_dim]\n",
        "\n",
        "        # Reshape to separate Q, K, V and heads\n",
        "        qkv = qkv.reshape(batch_size, seq_len, 3, self.num_heads, self.head_dim)\n",
        "        qkv = qkv.permute(2, 0, 3, 1, 4)  # Shape: [3, batch_size, num_heads, seq_len, head_dim]\n",
        "\n",
        "        # Unpack Q, K, V\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]  # Each shape: [batch_size, num_heads, seq_len, head_dim]\n",
        "\n",
        "        # Process each head's attention separately\n",
        "        attn_outputs = []\n",
        "\n",
        "        for h in range(self.num_heads):\n",
        "            q_h = q[:, h]  # [batch_size, seq_len, head_dim]\n",
        "            k_h = k[:, h]  # [batch_size, seq_len, head_dim]\n",
        "            v_h = v[:, h]  # [batch_size, seq_len, head_dim]\n",
        "\n",
        "            # Attention calculation for this head\n",
        "            attn_scores = torch.bmm(q_h, k_h.transpose(1, 2)) * (self.head_dim ** -0.5)\n",
        "            attn_weights = F.softmax(attn_scores, dim=-1)\n",
        "            attn_weights = self.dropout(attn_weights)\n",
        "            attn_output = torch.bmm(attn_weights, v_h)  # [batch_size, seq_len, head_dim]\n",
        "\n",
        "            attn_outputs.append(attn_output)\n",
        "\n",
        "        # Concatenate attention outputs from all heads\n",
        "        attn_output = torch.cat(attn_outputs, dim=2)  # [batch_size, seq_len, num_heads * head_dim]\n",
        "\n",
        "        # Project attention output\n",
        "        attn_output = self.attn_proj(attn_output)\n",
        "        attn_output = self.dropout(attn_output)\n",
        "\n",
        "        # Add residual connection and apply layer norm\n",
        "        x = self.norm1(residual + attn_output)\n",
        "\n",
        "        # MLP block with residual connection\n",
        "        residual = x\n",
        "        mlp_output = self.mlp(x)\n",
        "        x = self.norm2(residual + mlp_output)\n",
        "\n",
        "        # Mean pooling across patches for classification\n",
        "        x = x.mean(dim=1)  # Shape: [batch_size, embed_dim]\n",
        "\n",
        "        # Final classification layer\n",
        "        logits = self.classifier(x)\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "xWzPKZVRotfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# I'm just testing the forward pass here. For some reason all outputs come out as 7s\n",
        "# model = TransformerEncoderBlock(embed_dim=32).to(device)\n",
        "# dim_reducer = PatchDimReducer(in_features=49, out_features=32).to(device) #Move dim_reducer to the device\n",
        "# all_outputs = []\n",
        "\n",
        "# with torch.no_grad():  # No gradients needed for evaluation\n",
        "#     for images, labels in train_loader:\n",
        "#         images = images.to(device)\n",
        "#         patches = image_to_patches(images, patch_size=7)\n",
        "#         reduced_patches = dim_reducer(patches)\n",
        "\n",
        "#         # Get output for this batch\n",
        "#         batch_output = model(reduced_patches)\n",
        "#         all_outputs.append(batch_output.cpu())  # Move to CPU for concatenation\n",
        "#         break\n",
        "# # Combine all batch outputs\n",
        "# full_output = torch.cat(all_outputs, dim=0)\n",
        "# print(f\"Full output shape: {full_output.shape}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "941sGI_FJdlc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #Define Encoder Class\n",
        "# full_output.float().mean(dim = 1)"
      ],
      "metadata": {
        "id": "kS-mHkf9Dii_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # This is my training loop\n",
        "\n",
        "# # Initialize models\n",
        "# embed_dim = 32\n",
        "# patches_h = patches_w = 4  # 4×4 grid for MNIST with 7×7 patches\n",
        "\n",
        "# pos_embedding = LearnablePosition2DEmbedding(height=4, width=4, embed_dim=embed_dim).to(device)\n",
        "\n",
        "\n",
        "# model = TransformerEncoderBlock(embed_dim=32, num_heads=4).to(device)\n",
        "# dim_reducer = PatchDimReducer(in_features=49, out_features=32).to(device)\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# # Training loop with validation\n",
        "# num_epochs = 15\n",
        "\n",
        "# for epoch in range(num_epochs):\n",
        "#     # Training phase\n",
        "#     model.train()\n",
        "#     train_loss = 0.0\n",
        "#     train_correct = 0\n",
        "#     train_total = 0\n",
        "\n",
        "#     for images, labels in train_loader:\n",
        "#         images = images.to(device)\n",
        "#         labels = labels.to(device)\n",
        "\n",
        "#         # Process images into patches and reduce dimensions\n",
        "#         patches = image_to_patches(images, patch_size=7)\n",
        "#         reduced_patches = dim_reducer(patches).float()\n",
        "\n",
        "#         # Applying positional encoding\n",
        "#         reduced_patches = pos_embedding(reduced_patches)\n",
        "\n",
        "#         # Forward pass\n",
        "#         outputs = model(reduced_patches)\n",
        "\n",
        "#         # Compute loss\n",
        "#         loss = criterion(outputs, labels)\n",
        "\n",
        "#         # Backward and optimize\n",
        "#         optimizer.zero_grad()\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         # Track training statistics\n",
        "#         train_loss += loss.item()\n",
        "#         _, predicted = torch.max(outputs.data, 1)\n",
        "#         train_total += labels.size(0)\n",
        "#         train_correct += (predicted == labels).sum().item()\n",
        "\n",
        "#     # Calculate training metrics\n",
        "#     epoch_train_loss = train_loss / len(train_loader)\n",
        "#     epoch_train_acc = 100 * train_correct / train_total\n",
        "\n",
        "#     # Validation phase\n",
        "#     model.eval()\n",
        "#     val_loss = 0.0\n",
        "#     val_correct = 0\n",
        "#     val_total = 0\n",
        "\n",
        "#     with torch.no_grad():  # No need to track gradients during validation\n",
        "#         for images, labels in val_loader:\n",
        "#             images = images.to(device)\n",
        "#             labels = labels.to(device)\n",
        "\n",
        "#             # Process images\n",
        "#             patches = image_to_patches(images, patch_size=7)\n",
        "#             reduced_patches = dim_reducer(patches).float()\n",
        "\n",
        "#             # Add position embeddings (same as in training)\n",
        "#             reduced_patches = pos_embedding(reduced_patches)\n",
        "\n",
        "#             # Forward pass\n",
        "#             outputs = model(reduced_patches)\n",
        "\n",
        "#             # Compute validation loss\n",
        "#             loss = criterion(outputs, labels)\n",
        "#             val_loss += loss.item()\n",
        "\n",
        "#             # Track validation accuracy\n",
        "#             _, predicted = torch.max(outputs.data, 1)\n",
        "#             val_total += labels.size(0)\n",
        "#             val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "#     # Calculate validation metrics\n",
        "#     epoch_val_loss = val_loss / len(val_loader)\n",
        "#     epoch_val_acc = 100 * val_correct / val_total\n",
        "\n",
        "#     # Print epoch statistics\n",
        "#     print(f'Epoch {epoch+1}:')\n",
        "#     print(f'  Train Loss: {epoch_train_loss:.4f}, Train Accuracy: {epoch_train_acc:.2f}%')\n",
        "#     print(f'  Val Loss: {epoch_val_loss:.4f}, Val Accuracy: {epoch_val_acc:.2f}%')\n",
        "\n",
        "#     # Optional: Check for overfitting\n",
        "#     if epoch > 0 and epoch_train_acc - epoch_val_acc > 5:\n",
        "#         print(\"  Warning: Potential overfitting detected\")\n",
        "\n",
        "# # At the end of training loop cell:\n",
        "# last_val_loss = epoch_val_loss\n",
        "# last_val_acc = epoch_val_acc\n"
      ],
      "metadata": {
        "id": "JoCe-PZ6GpWZ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # After training is complete, save the model\n",
        "# model_path = \"/content/drive/MyDrive/VIT_MNIST_FourAttentionHead_SingleEncoder_LearnablePosition.pth\"\n",
        "\n",
        "# # Create a dictionary with model state and any other info you want to save\n",
        "# checkpoint = {\n",
        "#     'model_state_dict': model.state_dict(),\n",
        "#     'dim_reducer_state_dict': dim_reducer.state_dict(),\n",
        "#     'optimizer_state_dict': optimizer.state_dict(),\n",
        "#     'epoch': num_epochs,\n",
        "#     'embed_dim': 32,\n",
        "#     'patch_size': 7\n",
        "# }\n",
        "\n",
        "# # Save to Google Drive\n",
        "# torch.save(checkpoint, model_path)\n",
        "# print(f\"Model saved to {model_path}\")\n",
        "\n",
        "# # To load the model later:\n",
        "# \"\"\"\n",
        "# checkpoint = torch.load(model_path)\n",
        "# model = TransformerEncoderBlock(embed_dim=checkpoint['embed_dim']).to(device)\n",
        "# model.load_state_dict(checkpoint['model_state_dict'])\n",
        "# dim_reducer = PatchDimReducer(in_features=49, out_features=checkpoint['embed_dim']).to(device)\n",
        "# dim_reducer.load_state_dict(checkpoint['dim_reducer_state_dict'])\n",
        "# \"\"\""
      ],
      "metadata": {
        "id": "enc9TkB0INZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print(model)"
      ],
      "metadata": {
        "id": "jX4Kgr76sKVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PATCH TO IMAGE TEXT GENERATION TASK. HERE WE NEED TO COMBINE IMAGES TOGETHER AND THEN USE ENCODER + DECODER TO OUTPUT DIGITS ENCOUNTERED"
      ],
      "metadata": {
        "id": "8y3r8qFMZtaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder block class definition. This Encoder will not have a classifier head since the attached Decoder will be handling the output\n",
        "# start of with just a single encoder block and a single attention mechanism\n",
        "# with mult-head capabilities\n",
        "class TransformerEncoderBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, hidden_dim=128, dropout=0.1, num_heads=2, head_dim=12):\n",
        "        super().__init__()\n",
        "\n",
        "        # Store attention dimensions\n",
        "        self.num_heads = num_heads\n",
        "        #self.head_dim = head_dim\n",
        "        self.total_head_dim = num_heads * head_dim\n",
        "        self.head_dim = int(self.total_head_dim/num_heads)\n",
        "\n",
        "        # Combined QKV projection for all heads\n",
        "        self.qkv_proj = nn.Linear(embed_dim, int(3 * num_heads * head_dim))\n",
        "\n",
        "        # Output projection from attention\n",
        "        self.attn_proj = nn.Linear(int(num_heads * head_dim), embed_dim)\n",
        "\n",
        "        # Store dimensions for reshaping\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        # Layer normalization\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # MLP block\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embed_dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, embed_dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        # Classification head\n",
        "        #self.classifier = nn.Linear(embed_dim, 10).float()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Store original input for residual connection\n",
        "        residual = x\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "\n",
        "        # Combined QKV projection\n",
        "        qkv = self.qkv_proj(x)  # Shape: [batch_size, seq_len, 3 * num_heads * head_dim]\n",
        "\n",
        "        # Reshape to separate Q, K, V and heads\n",
        "        qkv = qkv.reshape(batch_size, seq_len, 3, self.num_heads, self.head_dim)\n",
        "        qkv = qkv.permute(2, 0, 3, 1, 4)  # Shape: [3, batch_size, num_heads, seq_len, head_dim]\n",
        "\n",
        "        # Unpack Q, K, V\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]  # Each shape: [batch_size, num_heads, seq_len, head_dim]\n",
        "\n",
        "        # Process each head's attention separately\n",
        "        attn_outputs = []\n",
        "\n",
        "        for h in range(self.num_heads):\n",
        "            q_h = q[:, h]  # [batch_size, seq_len, head_dim]\n",
        "            k_h = k[:, h]  # [batch_size, seq_len, head_dim]\n",
        "            v_h = v[:, h]  # [batch_size, seq_len, head_dim]\n",
        "\n",
        "            # Attention calculation for this head\n",
        "            attn_scores = torch.bmm(q_h, k_h.transpose(1, 2)) * (self.head_dim ** -0.5)\n",
        "            attn_weights = F.softmax(attn_scores, dim=-1)\n",
        "            attn_weights = self.dropout(attn_weights)\n",
        "            attn_output = torch.bmm(attn_weights, v_h)  # [batch_size, seq_len, head_dim]\n",
        "\n",
        "            attn_outputs.append(attn_output)\n",
        "\n",
        "        # Concatenate attention outputs from all heads\n",
        "        attn_output = torch.cat(attn_outputs, dim=2)  # [batch_size, seq_len, num_heads * head_dim]\n",
        "\n",
        "        # Project attention output\n",
        "        attn_output = self.attn_proj(attn_output)\n",
        "        attn_output = self.dropout(attn_output)\n",
        "\n",
        "        # Add residual connection and apply layer norm\n",
        "        x = self.norm1(residual + attn_output)\n",
        "\n",
        "        # MLP block with residual connection\n",
        "        residual = x\n",
        "        mlp_output = self.mlp(x)\n",
        "        x = self.norm2(residual + mlp_output)\n",
        "\n",
        "        # Mean pooling across patches for classification\n",
        "        #x = x.mean(dim=1)  # Shape: [batch_size, embed_dim]\n",
        "\n",
        "        # Final classification layer\n",
        "        #logits = self.classifier(x)\n",
        "\n",
        "        #return logits\n",
        "        return x"
      ],
      "metadata": {
        "id": "BOAMO5rOaCZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoder Block\n",
        "class TransformerDecoderBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, hidden_dim=128, dropout=0.1, num_heads=2, head_dim=12):\n",
        "        super().__init__()\n",
        "\n",
        "        # Store attention dimensions\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = head_dim\n",
        "        self.total_head_dim = num_heads * head_dim\n",
        "\n",
        "        # Masked Self-Attention\n",
        "        self.self_attn_norm = nn.LayerNorm(embed_dim).float()\n",
        "        self.self_attn_qkv = nn.Linear(embed_dim, 3 * num_heads * head_dim).float()\n",
        "        self.self_attn_proj = nn.Linear(num_heads * head_dim, embed_dim).float()\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "\n",
        "        # Cross-Attention\n",
        "        self.cross_attn_norm = nn.LayerNorm(embed_dim).float()\n",
        "        self.cross_attn_q = nn.Linear(embed_dim, num_heads * head_dim).float()\n",
        "        self.cross_attn_kv = nn.Linear(embed_dim, 2 * num_heads * head_dim).float()\n",
        "        self.cross_attn_proj = nn.Linear(num_heads * head_dim, embed_dim).float()\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "        # Feed-Forward Network\n",
        "        self.ffn_norm = nn.LayerNorm(embed_dim).float()\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(embed_dim, hidden_dim).float(),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, embed_dim).float(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        # Output projection\n",
        "        self.output_layer = nn.Linear(embed_dim, 13).float()  # 0-9 digits + SOS + EOS + PAD\n",
        "\n",
        "        # Store dimensions\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def forward(self, x, encoder_output, mask=None):\n",
        "        # x: [batch_size, seq_len, embed_dim]\n",
        "        # encoder_output: [batch_size, enc_seq_len, embed_dim]\n",
        "        # mask: [seq_len, seq_len] or None\n",
        "\n",
        "        batch_size, seq_len = x.shape[0], x.shape[1]\n",
        "\n",
        "        # 1. Masked Self-Attention\n",
        "        residual = x\n",
        "        x = self.self_attn_norm(x)\n",
        "\n",
        "        # Project to Q, K, V\n",
        "        qkv = self.self_attn_qkv(x)\n",
        "        qkv = qkv.reshape(batch_size, seq_len, 3, self.num_heads, self.head_dim)\n",
        "        qkv = qkv.permute(2, 0, 3, 1, 4)  # [3, batch_size, num_heads, seq_len, head_dim]\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        # Process each head's attention\n",
        "        attn_outputs = []\n",
        "        for h in range(self.num_heads):\n",
        "            q_h = q[:, h]  # [batch_size, seq_len, head_dim]\n",
        "            k_h = k[:, h]  # [batch_size, seq_len, head_dim]\n",
        "            v_h = v[:, h]  # [batch_size, seq_len, head_dim]\n",
        "\n",
        "            # Calculate attention scores\n",
        "            attn_scores = torch.bmm(q_h, k_h.transpose(1, 2)) * (self.head_dim ** -0.5)\n",
        "\n",
        "            # Apply causal mask if provided\n",
        "            if mask is not None:\n",
        "                attn_scores = attn_scores + mask\n",
        "\n",
        "            # Apply softmax and dropout\n",
        "            attn_weights = F.softmax(attn_scores, dim=-1)\n",
        "            attn_weights = self.dropout1(attn_weights)\n",
        "            attn_output = torch.bmm(attn_weights, v_h)\n",
        "\n",
        "            attn_outputs.append(attn_output)\n",
        "\n",
        "        # Concatenate outputs from all heads\n",
        "        self_attn_output = torch.cat(attn_outputs, dim=2)\n",
        "        self_attn_output = self.self_attn_proj(self_attn_output)\n",
        "        self_attn_output = self.dropout1(self_attn_output)\n",
        "\n",
        "        # Add residual connection\n",
        "        x = residual + self_attn_output\n",
        "\n",
        "        # 2. Cross-Attention\n",
        "        residual = x\n",
        "        x = self.cross_attn_norm(x)\n",
        "\n",
        "        # Project decoder sequence to queries\n",
        "        q = self.cross_attn_q(x)\n",
        "        q = q.reshape(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "        q = q.permute(0, 2, 1, 3)  # [batch_size, num_heads, seq_len, head_dim]\n",
        "\n",
        "        # Project encoder outputs to keys and values\n",
        "        kv = self.cross_attn_kv(encoder_output)\n",
        "        enc_seq_len = encoder_output.size(1)\n",
        "        kv = kv.reshape(batch_size, enc_seq_len, 2, self.num_heads, self.head_dim)\n",
        "        kv = kv.permute(2, 0, 3, 1, 4)  # [2, batch_size, num_heads, enc_seq_len, head_dim]\n",
        "        k, v = kv[0], kv[1]\n",
        "\n",
        "        # Process each head's cross-attention\n",
        "        cross_attn_outputs = []\n",
        "        for h in range(self.num_heads):\n",
        "            q_h = q[:, h]  # [batch_size, seq_len, head_dim]\n",
        "            k_h = k[:, h]  # [batch_size, enc_seq_len, head_dim]\n",
        "            v_h = v[:, h]  # [batch_size, enc_seq_len, head_dim]\n",
        "\n",
        "            # Calculate cross-attention scores\n",
        "            cross_attn_scores = torch.bmm(q_h, k_h.transpose(1, 2)) * (self.head_dim ** -0.5)\n",
        "\n",
        "            # Apply softmax and dropout\n",
        "            cross_attn_weights = F.softmax(cross_attn_scores, dim=-1)\n",
        "            cross_attn_weights = self.dropout2(cross_attn_weights)\n",
        "            cross_attn_output = torch.bmm(cross_attn_weights, v_h)\n",
        "\n",
        "            cross_attn_outputs.append(cross_attn_output)\n",
        "\n",
        "        # Concatenate outputs from all heads\n",
        "        cross_attn_output = torch.cat(cross_attn_outputs, dim=2)\n",
        "        cross_attn_output = self.cross_attn_proj(cross_attn_output)\n",
        "        cross_attn_output = self.dropout2(cross_attn_output)\n",
        "\n",
        "        # Add residual connection\n",
        "        x = residual + cross_attn_output\n",
        "\n",
        "        # 3. Feed-Forward Network\n",
        "        residual = x\n",
        "        x = self.ffn_norm(x)\n",
        "        x = residual + self.ffn(x)\n",
        "\n",
        "        # 4. Output projection (vocabulary distribution)\n",
        "        logits = self.output_layer(x)\n",
        "\n",
        "        return logits"
      ],
      "metadata": {
        "id": "n8lJRwZkU1XB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class StackedTransformerEncoder(nn.Module):\n",
        "    def __init__(self, embed_dim, num_layers=2, **kwargs):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerEncoderBlock(embed_dim, **kwargs)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)  # Each layer processes the output of the previous\n",
        "        return x\n",
        "\n",
        "class StackedTransformerDecoder(nn.Module):\n",
        "    def __init__(self, embed_dim, num_layers=2, **kwargs):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerDecoderBlock(embed_dim, **kwargs)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Add a projection layer to convert between layers if this is a multi-layer setup\n",
        "        self.intermediate_projections = nn.ModuleList([\n",
        "            nn.Linear(13, embed_dim).float()  # 13 is your vocab size\n",
        "            for _ in range(num_layers-1)\n",
        "        ]) if num_layers > 1 else nn.ModuleList([])\n",
        "\n",
        "    def forward(self, x, encoder_output, mask=None):\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            # Process through the current decoder layer\n",
        "            output = layer(x, encoder_output, mask)\n",
        "\n",
        "            # If this isn't the last layer, project back to embedding space\n",
        "            if i < len(self.layers) - 1:\n",
        "                x = self.intermediate_projections[i](output)\n",
        "            else:\n",
        "                x = output\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "HWoIoAM1--iY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Full Seq2Seq Model\n",
        "class DigitSequenceTransformer(nn.Module):\n",
        "    def __init__(self, encoder, decoder, max_seq_len=6, vocab_size=13, embed_dim=32):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "        # Add token embedding layer\n",
        "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "        # Constants for special tokens\n",
        "        self.SOS_TOKEN = 10\n",
        "        self.EOS_TOKEN = 11\n",
        "        self.PAD_TOKEN = 12\n",
        "\n",
        "    def forward(self, images, decoder_input=None):\n",
        "        # Encode the images\n",
        "        encoder_output = self.encoder(images)\n",
        "\n",
        "        # If decoder_input is provided (training mode)\n",
        "        if decoder_input is not None:\n",
        "            # Embed the decoder input tokens\n",
        "            decoder_emb = self.token_embedding(decoder_input)\n",
        "\n",
        "            # Create causal mask for decoder self-attention\n",
        "            seq_len = decoder_input.size(1)\n",
        "            mask = torch.triu(torch.ones(seq_len, seq_len) * float('-inf'), diagonal=1).to(images.device)\n",
        "\n",
        "            # Decode with teacher forcing\n",
        "            decoder_output = self.decoder(decoder_emb, encoder_output, mask)\n",
        "            return decoder_output\n",
        "\n",
        "        # If no decoder_input (inference mode)\n",
        "        else:\n",
        "            # Start with batch of SOS tokens\n",
        "            batch_size = images.size(0)\n",
        "            current_seq = torch.full((batch_size, 1), self.SOS_TOKEN).to(images.device)\n",
        "\n",
        "            # Store predictions\n",
        "            all_preds = []\n",
        "\n",
        "            # Generate sequence auto-regressively\n",
        "            for i in range(self.max_seq_len - 1):\n",
        "                # Create appropriate causal mask\n",
        "                seq_len = current_seq.size(1)\n",
        "                mask = torch.triu(torch.ones(seq_len, seq_len) * float('-inf'), diagonal=1).to(images.device)\n",
        "\n",
        "                # Embed tokens\n",
        "                current_emb = self.token_embedding(current_seq)\n",
        "\n",
        "                # Forward pass through decoder\n",
        "                decoder_output = self.decoder(current_emb, encoder_output, mask)\n",
        "\n",
        "                # Get prediction for next token\n",
        "                next_token_logits = decoder_output[:, -1, :]\n",
        "                next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
        "\n",
        "                # Save predictions\n",
        "                all_preds.append(next_token)\n",
        "\n",
        "                # Append next token to sequence\n",
        "                current_seq = torch.cat([current_seq, next_token], dim=1)\n",
        "\n",
        "                # Check if all sequences have generated EOS\n",
        "                if (next_token == self.EOS_TOKEN).all():\n",
        "                    break\n",
        "\n",
        "            # Concatenate all predictions\n",
        "            predictions = torch.cat(all_preds, dim=1)\n",
        "            return predictions\n"
      ],
      "metadata": {
        "id": "3xnOIIwqVtIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define vocabulary\n",
        "fusion_factor = 4\n",
        "DIGIT_TOKENS = list(range(10))  # 0-9\n",
        "SOS_TOKEN = 10\n",
        "EOS_TOKEN = 11\n",
        "PAD_TOKEN = 12\n",
        "vocab_size = 13\n",
        "max_seq_len = fusion_factor + 2"
      ],
      "metadata": {
        "id": "fnBjuc-fRd1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_target_sequence(digit_lists, max_seq_len):\n",
        "    \"\"\"Convert list of digit lists to target sequences with EOS and padding\"\"\"\n",
        "    batch_size = len(digit_lists)\n",
        "    target_seq = torch.full((batch_size, max_seq_len), PAD_TOKEN)\n",
        "\n",
        "    for i, digits in enumerate(digit_lists):\n",
        "        # Add digits followed by EOS\n",
        "        seq = torch.tensor(digits + [EOS_TOKEN])\n",
        "        seq_len = len(seq)\n",
        "        target_seq[i, :seq_len] = seq\n",
        "\n",
        "    return target_seq"
      ],
      "metadata": {
        "id": "jN9nC0kRRkRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fuse_images(images, fusion_factor):\n",
        "    \"\"\"\n",
        "    Fuse multiple images into a single larger image in a square grid.\n",
        "\n",
        "    Args:\n",
        "        images: Tensor of shape [batch_size, channels, height, width]\n",
        "        fusion_factor: Number of images to fuse (must be 1, 4, or 16)\n",
        "\n",
        "    Returns:\n",
        "        Tensor of fused images with shape [batch_size/fusion_factor, channels, height*sqrt(fusion_factor), width*sqrt(fusion_factor)]\n",
        "    \"\"\"\n",
        "    # Validate fusion factor is a perfect square\n",
        "    grid_side = int(math.sqrt(fusion_factor))\n",
        "    assert grid_side * grid_side == fusion_factor, \"Fusion factor must be a perfect square (1, 4, 16, etc.)\"\n",
        "\n",
        "    batch_size, channels, height, width = images.shape\n",
        "\n",
        "    # Ensure batch size is divisible by fusion factor\n",
        "    assert batch_size % fusion_factor == 0, f\"Batch size ({batch_size}) must be divisible by fusion factor ({fusion_factor})\"\n",
        "\n",
        "    # Calculate new dimensions\n",
        "    new_batch_size = batch_size // fusion_factor\n",
        "    new_height = height * grid_side\n",
        "    new_width = width * grid_side\n",
        "\n",
        "    # Reshape the batch to prepare for fusion\n",
        "    # First reshape to [new_batch_size, fusion_factor, channels, height, width]\n",
        "    reshaped = images.view(new_batch_size, fusion_factor, channels, height, width)\n",
        "\n",
        "    # Further reshape to [new_batch_size, grid_side, grid_side, channels, height, width]\n",
        "    reshaped = reshaped.view(new_batch_size, grid_side, grid_side, channels, height, width)\n",
        "\n",
        "    # Permute and reshape to arrange images in a grid\n",
        "    # [new_batch_size, grid_side, grid_side, channels, height, width] ->\n",
        "    # [new_batch_size, channels, grid_side, height, grid_side, width]\n",
        "    permuted = reshaped.permute(0, 3, 1, 4, 2, 5)\n",
        "\n",
        "    # Reshape to final output shape\n",
        "    # [new_batch_size, channels, grid_side*height, grid_side*width]\n",
        "    fused = permuted.reshape(new_batch_size, channels, new_height, new_width)\n",
        "\n",
        "    return fused"
      ],
      "metadata": {
        "id": "rJeO8U2_ackO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_decoder_input(digit_lists, max_seq_len):\n",
        "    \"\"\"Create decoder inputs (SOS + digits, right-padded)\"\"\"\n",
        "    batch_size = len(digit_lists)\n",
        "    decoder_input = torch.full((batch_size, max_seq_len), PAD_TOKEN)\n",
        "\n",
        "    for i, digits in enumerate(digit_lists):\n",
        "        # Add SOS followed by digits\n",
        "        seq = torch.tensor([SOS_TOKEN] + digits)\n",
        "        seq_len = len(seq)\n",
        "        decoder_input[i, :seq_len] = seq\n",
        "\n",
        "    return decoder_input"
      ],
      "metadata": {
        "id": "6gH2UU-RSFmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training function\n",
        "def train_seq2seq(model, dim_reducer, pos_embedding, train_loader, val_loader,\n",
        "                 criterion, optimizer, num_epochs=2, fusion_factor=4,\n",
        "                 patch_size=7, device='cuda'):\n",
        "\n",
        "    # Define special tokens\n",
        "    SOS_TOKEN = 10\n",
        "    EOS_TOKEN = 11\n",
        "    PAD_TOKEN = 12\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "            batch_size = images.shape[0]\n",
        "\n",
        "            # Skip incomplete batches for fusion\n",
        "            if batch_size % fusion_factor != 0:\n",
        "                continue\n",
        "\n",
        "            # Fuse images\n",
        "            fused_images = fuse_images(images, fusion_factor).to(device)\n",
        "\n",
        "            # Create digit sequences from labels\n",
        "            digit_lists = []\n",
        "            for i in range(0, batch_size, fusion_factor):\n",
        "                digit_lists.append(labels[i:i+fusion_factor].tolist())\n",
        "\n",
        "            # Create target sequences and decoder inputs\n",
        "            max_seq_len = fusion_factor + 2  # digits + EOS + padding\n",
        "            target_seq = create_target_sequence(digit_lists, max_seq_len).to(device)\n",
        "            decoder_input = create_decoder_input(digit_lists, max_seq_len).to(device)\n",
        "\n",
        "            # Process images for encoder\n",
        "            patches = image_to_patches(fused_images, patch_size=patch_size)\n",
        "            reduced_patches = dim_reducer(patches).float()\n",
        "            reduced_patches = pos_embedding(reduced_patches)\n",
        "\n",
        "            # Forward pass\n",
        "            decoder_output = model(reduced_patches, decoder_input)\n",
        "\n",
        "            # Calculate loss (ignore padding)\n",
        "            loss = criterion(\n",
        "                decoder_output.reshape(-1, 13),  # 13 = vocab_size\n",
        "                target_seq.reshape(-1),\n",
        "                #ignore_index=PAD_TOKEN\n",
        "            )\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Track training statistics\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            # Calculate token accuracy (excluding padding)\n",
        "            pred = decoder_output.argmax(dim=2)\n",
        "            mask = (target_seq != PAD_TOKEN)\n",
        "            correct = ((pred == target_seq) & mask).sum().item()\n",
        "            total = mask.sum().item()\n",
        "            train_correct += correct\n",
        "            train_total += total\n",
        "\n",
        "        # Calculate training metrics\n",
        "        epoch_train_loss = train_loss / len(train_loader)\n",
        "        epoch_train_acc = 100 * train_correct / train_total if train_total > 0 else 0\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (images, labels) in enumerate(val_loader):\n",
        "                batch_size = images.shape[0]\n",
        "\n",
        "                # Skip incomplete batches\n",
        "                if batch_size % fusion_factor != 0:\n",
        "                    continue\n",
        "\n",
        "                # Fuse images\n",
        "                fused_images = fuse_images(images, fusion_factor).to(device)\n",
        "\n",
        "                # Create digit sequences from labels\n",
        "                digit_lists = []\n",
        "                for i in range(0, batch_size, fusion_factor):\n",
        "                    digit_lists.append(labels[i:i+fusion_factor].tolist())\n",
        "\n",
        "                # Create target sequences and decoder inputs\n",
        "                max_seq_len = fusion_factor + 2\n",
        "                target_seq = create_target_sequence(digit_lists, max_seq_len).to(device)\n",
        "                decoder_input = create_decoder_input(digit_lists, max_seq_len).to(device)\n",
        "\n",
        "                # Process images for encoder\n",
        "                patches = image_to_patches(fused_images, patch_size=patch_size)\n",
        "                reduced_patches = dim_reducer(patches).float()\n",
        "                reduced_patches = pos_embedding(reduced_patches)\n",
        "\n",
        "                # Forward pass\n",
        "                decoder_output = model(reduced_patches, decoder_input)\n",
        "\n",
        "                # Calculate validation loss\n",
        "                loss = criterion(\n",
        "                    decoder_output.reshape(-1, 13),\n",
        "                    target_seq.reshape(-1),\n",
        "                    #ignore_index=PAD_TOKEN\n",
        "                )\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                # Calculate token accuracy\n",
        "                pred = decoder_output.argmax(dim=2)\n",
        "                mask = (target_seq != PAD_TOKEN)\n",
        "                correct = ((pred == target_seq) & mask).sum().item()\n",
        "                total = mask.sum().item()\n",
        "                val_correct += correct\n",
        "                val_total += total\n",
        "\n",
        "        # Calculate validation metrics\n",
        "        epoch_val_loss = val_loss / len(val_loader)\n",
        "        epoch_val_acc = 100 * val_correct / val_total if val_total > 0 else 0\n",
        "\n",
        "        # Print epoch statistics\n",
        "        print(f'Epoch {epoch+1}:')\n",
        "        print(f'  Train Loss: {epoch_train_loss:.4f}, Train Token Accuracy: {epoch_train_acc:.2f}%')\n",
        "        print(f'  Val Loss: {epoch_val_loss:.4f}, Val Token Accuracy: {epoch_val_acc:.2f}%')\n"
      ],
      "metadata": {
        "id": "fL5ieQAbWH6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# This is where we train the full Seq2Seq Model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# Hyperparameters\n",
        "embed_dim = 32\n",
        "hidden_dim = 128\n",
        "num_heads = 4\n",
        "head_dim = 32\n",
        "dropout = 0.1\n",
        "patch_size = 7\n",
        "fusion_factor = 4\n",
        "batch_size = 16\n",
        "num_epochs = 10\n",
        "learning_rate = 0.0001\n",
        "\n",
        "# Calculate height and width of patch grid after fusion\n",
        "if fusion_factor == 1:\n",
        "    # Standard MNIST - 4×4 grid of patches (16 total)\n",
        "    patch_height = 4\n",
        "    patch_width = 4\n",
        "elif fusion_factor == 4:\n",
        "    # 2×2 grid of MNIST images = 8×8 grid of patches (64 total)\n",
        "    patch_height = 8\n",
        "    patch_width = 8\n",
        "elif fusion_factor == 16:\n",
        "    # 4×4 grid of MNIST images = 16×16 grid of patches (256 total)\n",
        "    patch_height = 16\n",
        "    patch_width = 16\n",
        "else:\n",
        "    raise ValueError(f\"Unsupported fusion_factor: {fusion_factor}\")\n",
        "\n",
        "print(f\"Patch grid dimensions: {patch_height}×{patch_width} = {patch_height*patch_width} total patches\")\n",
        "\n",
        "# Initialize models\n",
        "encoder = StackedTransformerEncoder(\n",
        "    embed_dim=embed_dim,\n",
        "    num_layers=2,\n",
        "    hidden_dim=hidden_dim,\n",
        "    dropout=dropout,\n",
        "    num_heads=num_heads,\n",
        "    head_dim=head_dim\n",
        ").to(device)\n",
        "\n",
        "decoder = StackedTransformerDecoder(\n",
        "    embed_dim=embed_dim,\n",
        "    num_layers=2,\n",
        "    hidden_dim=hidden_dim,\n",
        "    dropout=dropout,\n",
        "    num_heads=num_heads,\n",
        "    head_dim=head_dim\n",
        ").to(device)\n",
        "\n",
        "# Create full seq2seq model\n",
        "model = DigitSequenceTransformer(\n",
        "    encoder=encoder,\n",
        "    decoder=decoder,\n",
        "    max_seq_len=fusion_factor + 2,  # digits + EOS + potential extra\n",
        "    vocab_size=13,  # 0-9 digits + SOS + EOS + PAD\n",
        "    embed_dim=embed_dim\n",
        ").to(device)\n",
        "\n",
        "# Initialize dimension reducer for patches\n",
        "dim_reducer = PatchDimReducer(\n",
        "    in_features=patch_size * patch_size,\n",
        "    out_features=embed_dim\n",
        ").to(device)\n",
        "\n",
        "# Initialize position embedding with correct dimensions\n",
        "pos_embedding = LearnablePosition2DEmbedding(\n",
        "    height=patch_height,\n",
        "    width=patch_width,\n",
        "    embed_dim=embed_dim\n",
        ").to(device)\n",
        "\n",
        "# Load data loaders\n",
        "train_loader, val_loader, test_loader = load_and_split_mnist(\n",
        "    validation_split=0.1,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=model.PAD_TOKEN)\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(model.parameters()) +\n",
        "    list(dim_reducer.parameters()) +\n",
        "    list(pos_embedding.parameters()),\n",
        "    lr=learning_rate\n",
        ")\n",
        "\n",
        "# Optional: learning rate scheduler\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer,\n",
        "    mode='min',\n",
        "    factor=0.5,\n",
        "    patience=3,\n",
        "    #verbose=True\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "print(f\"Starting training with fusion_factor={fusion_factor}, num_heads={num_heads}\")\n",
        "train_seq2seq(\n",
        "    model=model,\n",
        "    dim_reducer=dim_reducer,\n",
        "    pos_embedding=pos_embedding,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    num_epochs=num_epochs,\n",
        "    fusion_factor=fusion_factor,\n",
        "    patch_size=patch_size,\n",
        "    device=device\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGc_3plgXzhl",
        "outputId": "92684872-7142-4f70-98fa-5d88df6b681d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Patch grid dimensions: 8×8 = 64 total patches\n",
            "Starting training with fusion_factor=4, num_heads=4\n",
            "Epoch 1:\n",
            "  Train Loss: 1.9685, Train Token Accuracy: 30.78%\n",
            "  Val Loss: 1.7347, Val Token Accuracy: 37.72%\n",
            "Epoch 2:\n",
            "  Train Loss: 1.7829, Train Token Accuracy: 37.16%\n",
            "  Val Loss: 1.5733, Val Token Accuracy: 44.79%\n",
            "Epoch 3:\n",
            "  Train Loss: 1.5484, Train Token Accuracy: 46.17%\n",
            "  Val Loss: 1.1260, Val Token Accuracy: 62.49%\n",
            "Epoch 4:\n",
            "  Train Loss: 1.1860, Train Token Accuracy: 59.24%\n",
            "  Val Loss: 0.8046, Val Token Accuracy: 74.08%\n",
            "Epoch 5:\n",
            "  Train Loss: 0.9448, Train Token Accuracy: 68.02%\n",
            "  Val Loss: 0.6052, Val Token Accuracy: 80.43%\n",
            "Epoch 6:\n",
            "  Train Loss: 0.8016, Train Token Accuracy: 73.16%\n",
            "  Val Loss: 0.4965, Val Token Accuracy: 84.39%\n",
            "Epoch 7:\n",
            "  Train Loss: 0.6919, Train Token Accuracy: 77.04%\n",
            "  Val Loss: 0.4085, Val Token Accuracy: 87.03%\n",
            "Epoch 8:\n",
            "  Train Loss: 0.6057, Train Token Accuracy: 80.31%\n",
            "  Val Loss: 0.3450, Val Token Accuracy: 89.28%\n",
            "Epoch 9:\n",
            "  Train Loss: 0.5281, Train Token Accuracy: 82.96%\n",
            "  Val Loss: 0.3004, Val Token Accuracy: 90.85%\n",
            "Epoch 10:\n",
            "  Train Loss: 0.4616, Train Token Accuracy: 85.40%\n",
            "  Val Loss: 0.2660, Val Token Accuracy: 91.95%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a single batch and run the trained model in inference mode\n",
        "fusion_factor = 4\n",
        "dataiter = iter(test_loader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# Calculate height and width of patch grid after fusion\n",
        "if fusion_factor == 1:\n",
        "    # Standard MNIST - 4×4 grid of patches (16 total)\n",
        "    patch_height = 4\n",
        "    patch_width = 4\n",
        "elif fusion_factor == 4:\n",
        "    # 2×2 grid of MNIST images = 8×8 grid of patches (64 total)\n",
        "    patch_height = 8\n",
        "    patch_width = 8\n",
        "elif fusion_factor == 16:\n",
        "    # 4×4 grid of MNIST images = 16×16 grid of patches (256 total)\n",
        "    patch_height = 16\n",
        "    patch_width = 16\n",
        "else:\n",
        "    raise ValueError(f\"Unsupported fusion_factor: {fusion_factor}\")\n",
        "\n",
        "# Initialize position embedding with correct dimensions\n",
        "pos_embedding = LearnablePosition2DEmbedding(\n",
        "    height=patch_height,  # Use calculated patch_height\n",
        "    width=patch_width,  # Use calculated patch_width\n",
        "    embed_dim=embed_dim\n",
        ").to(device)\n",
        "\n",
        "# Filter to ensure batch size is divisible by fusion factor\n",
        "batch_size = images.shape[0]\n",
        "if batch_size % fusion_factor != 0:\n",
        "    # Trim to nearest multiple of fusion_factor\n",
        "    images = images[:batch_size - (batch_size % fusion_factor)]\n",
        "    labels = labels[:batch_size - (batch_size % fusion_factor)]\n",
        "\n",
        "# Prepare the input\n",
        "fused_images = fuse_images(images, fusion_factor).to(device)\n",
        "patches = image_to_patches(fused_images, patch_size=7)\n",
        "reduced_patches = dim_reducer(patches).float()\n",
        "reduced_patches = pos_embedding(reduced_patches)\n",
        "\n",
        "# Run inference\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    predictions = model(reduced_patches)\n",
        "\n",
        "# Display results\n",
        "for i in range(fused_images.size(0)):\n",
        "    # Get original labels\n",
        "    original_indices = list(range(i*fusion_factor, (i+1)*fusion_factor))\n",
        "    original_digits = [labels[j].item() for j in original_indices]\n",
        "\n",
        "    # Get predicted sequence (remove any EOS/PAD tokens)\n",
        "    pred_sequence = []\n",
        "    for token in predictions[i].cpu().tolist():\n",
        "        if token == model.EOS_TOKEN:\n",
        "            pred_sequence.append(\"EOS\")\n",
        "            break\n",
        "        elif token == model.PAD_TOKEN:\n",
        "            pred_sequence.append(\"PAD\")\n",
        "        else:\n",
        "            pred_sequence.append(str(token))\n",
        "\n",
        "    print(f\"Image {i}:\")\n",
        "    print(f\"  Original digits: {original_digits}\")\n",
        "    print(f\"  Predicted sequence: {pred_sequence}\")\n",
        "\n",
        "    # Optionally display the image\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.imshow(fused_images[i].cpu().permute(1, 2, 0).squeeze(), cmap='gray')\n",
        "    plt.title(f\"Original: {original_digits}\\nPredicted: {pred_sequence}\")\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8KWgn7n5lKWh",
        "outputId": "5713058c-0a6b-4d57-afe6-208aed8f1dca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image 0:\n",
            "  Original digits: [7, 2, 1, 0]\n",
            "  Predicted sequence: ['4', '4', '4', '4', '4']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image 1:\n",
            "  Original digits: [4, 1, 4, 9]\n",
            "  Predicted sequence: ['4', '4', '4', '4', '4']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image 2:\n",
            "  Original digits: [5, 9, 0, 6]\n",
            "  Predicted sequence: ['8', '4', '8', '4', 'EOS']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image 3:\n",
            "  Original digits: [9, 0, 1, 5]\n",
            "  Predicted sequence: ['4', '8', '4', '4', 'EOS']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# After training is complete, save the model\n",
        "model_path = \"/content/drive/MyDrive/VIT_MNIST_FourHead_EncoderDecoder_20Epochs.pth\"\n",
        "\n",
        "# Create a dictionary with model state and any other info you want to save\n",
        "checkpoint = {\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'dim_reducer_state_dict': dim_reducer.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'epoch': num_epochs,\n",
        "    'embed_dim': 32,\n",
        "    'patch_size': 7\n",
        "}\n",
        "\n",
        "# Save to Google Drive\n",
        "torch.save(checkpoint, model_path)\n",
        "print(f\"Model saved to {model_path}\")\n",
        "\n",
        "# To load the model later:\n",
        "\"\"\"\n",
        "checkpoint = torch.load(model_path)\n",
        "model = TransformerEncoderBlock(embed_dim=checkpoint['embed_dim']).to(device)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "dim_reducer = PatchDimReducer(in_features=49, out_features=checkpoint['embed_dim']).to(device)\n",
        "dim_reducer.load_state_dict(checkpoint['dim_reducer_state_dict'])\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "oURncPY-4rLm",
        "outputId": "1aa7ad77-3e03-4cf1-b5e2-93f52bb8a968"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to /content/drive/MyDrive/VIT_MNIST_FourHead_EncoderDecoder_20Epochs.pth\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\ncheckpoint = torch.load(model_path)\\nmodel = TransformerEncoderBlock(embed_dim=checkpoint['embed_dim']).to(device)\\nmodel.load_state_dict(checkpoint['model_state_dict'])\\ndim_reducer = PatchDimReducer(in_features=49, out_features=checkpoint['embed_dim']).to(device)\\ndim_reducer.load_state_dict(checkpoint['dim_reducer_state_dict'])\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test function\n",
        "raise SystemExit(\"Skipping this cell.\")\n",
        "\n",
        "def test_seq2seq(model, dim_reducer, pos_embedding, test_loader,\n",
        "                fusion_factor=4, patch_size=7, device='cuda'):\n",
        "\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (images, labels) in enumerate(test_loader):\n",
        "            batch_size = images.shape[0]\n",
        "\n",
        "            # Skip incomplete batches\n",
        "            if batch_size % fusion_factor != 0:\n",
        "                continue\n",
        "\n",
        "            # Fuse images\n",
        "            fused_images = fuse_images(images, fusion_factor).to(device)\n",
        "\n",
        "            # Create digit lists from labels for reference\n",
        "            digit_lists = []\n",
        "            for i in range(0, batch_size, fusion_factor):\n",
        "                digit_lists.append(labels[i:i+fusion_factor].tolist())\n",
        "\n",
        "            # Process images for encoder\n",
        "            patches = image_to_patches(fused_images, patch_size=patch_size)\n",
        "            reduced_patches = dim_reducer(patches).float()\n",
        "            reduced_patches = pos_embedding(reduced_patches)\n",
        "\n",
        "            # Generate sequences (inference mode without teacher forcing)\n",
        "            predictions = model(reduced_patches)\n",
        "\n",
        "            # Store results for evaluation\n",
        "            for i, pred_seq in enumerate(predictions):\n",
        "                # Convert to list and stop at EOS token\n",
        "                pred_list = []\n",
        "                for token in pred_seq.cpu().tolist():\n",
        "                    if token == model.EOS_TOKEN:\n",
        "                        break\n",
        "                    pred_list.append(token)\n",
        "\n",
        "                all_predictions.append(pred_list)\n",
        "                all_targets.append(digit_lists[i])\n",
        "\n",
        "    # Calculate sequence accuracy (exact match)\n",
        "    correct = sum(1 for pred, target in zip(all_predictions, all_targets) if pred == target)\n",
        "    total = len(all_predictions)\n",
        "    sequence_accuracy = 100 * correct / total if total > 0 else 0\n",
        "\n",
        "    # Calculate digit accuracy\n",
        "    digit_correct = 0\n",
        "    digit_total = 0\n",
        "    for pred, target in zip(all_predictions, all_targets):\n",
        "        # Compare up to the length of the shorter sequence\n",
        "        min_len = min(len(pred), len(target))\n",
        "        digit_correct += sum(1 for i in range(min_len) if pred[i] == target[i])\n",
        "        digit_total += max(len(pred), len(target))  # Count extra or missing digits as errors\n",
        "\n",
        "    digit_accuracy = 100 * digit_correct / digit_total if digit_total > 0 else 0\n",
        "\n",
        "    print(f'Test Results:')\n",
        "    print(f'  Sequence Accuracy: {sequence_accuracy:.2f}%')\n",
        "    print(f'  Digit Accuracy: {digit_accuracy:.2f}%')\n",
        "\n",
        "    # Show some examples\n",
        "    print(\"\\nExample predictions:\")\n",
        "    for i in range(min(5, len(all_predictions))):\n",
        "        print(f\"Target: {all_targets[i]}, Prediction: {all_predictions[i]}\")\n",
        "\n",
        "    return sequence_accuracy, digit_accuracy"
      ],
      "metadata": {
        "id": "Zq2v7il5WRoU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}